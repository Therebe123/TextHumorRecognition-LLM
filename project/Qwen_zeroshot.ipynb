{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 前期准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579756c0904245c49013f43a568d6a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "model_path = \"../.cache/modelscope/hub/qwen/Qwen1.5-7B-Chat-GPTQ-Int4/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"auto\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'gate_proj', 'k_proj', 'q_proj', 'o_proj', 'down_proj', 'v_proj', 'up_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    inference_mode=False,  # 训练模式\n",
    "    r=8,  # Lora 秩\n",
    "    lora_alpha=32,  # Lora alaph\n",
    "    lora_dropout=0.1,  # Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19,988,480 || all params: 1,264,914,432 || trainable%: 1.580223886638365\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "      <th>label</th>\n",
       "      <th>len_joke</th>\n",
       "      <th>标签</th>\n",
       "      <th>pinyin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>车上遇见一对双胞胎姑娘。穿的那个清凉啊，笑的那个甜啊……忍不住要去搭讪。 为不表现的轻浮和唐...</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>强幽默</td>\n",
       "      <td>che1 shang4 yu4 jian4 yi1 dui4 shuang1 bao1 ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>图书馆自习室每个桌面上都摆满了占座的书，很是让其他人气愤。 某生不占座没有固定的座位，一日去...</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>弱幽默</td>\n",
       "      <td>tu2 shu1 guan3 zi4 xi2 shi4 mei3 ge4 zhuo1 mia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一个报童在大街上高声叫卖：骇人听闻的诈骗案，受害者多达82人！ 某行人连忙上前买一份。可是，...</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>强幽默</td>\n",
       "      <td>yi1 ge4 bao4 tong2 zai4 da4 jie1 shang4 gao1 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>法官对被告说：你不但偷钱，还拿了表，戒指和珍珠。 被告说：是的，法官先生，人们不是常说‘光有...</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>强幽默</td>\n",
       "      <td>fa3 guan1 dui4 bei4 gao4 shuo1 ： ni3 bu4 dan4 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>大人，原告在法庭上说，这个人同我一起生活了几天，答应同我结婚，可是后来他同别的女人结了婚。他...</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>一般幽默</td>\n",
       "      <td>da4 ren2 ， yuan2 gao4 zai4 fa3 ting2 shang4 sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                joke  label  len_joke    标签  \\\n",
       "0  车上遇见一对双胞胎姑娘。穿的那个清凉啊，笑的那个甜啊……忍不住要去搭讪。 为不表现的轻浮和唐...      2        83   强幽默   \n",
       "1  图书馆自习室每个桌面上都摆满了占座的书，很是让其他人气愤。 某生不占座没有固定的座位，一日去...      0       192   弱幽默   \n",
       "2  一个报童在大街上高声叫卖：骇人听闻的诈骗案，受害者多达82人！ 某行人连忙上前买一份。可是，...      2       102   强幽默   \n",
       "3  法官对被告说：你不但偷钱，还拿了表，戒指和珍珠。 被告说：是的，法官先生，人们不是常说‘光有...      2        56   强幽默   \n",
       "4  大人，原告在法庭上说，这个人同我一起生活了几天，答应同我结婚，可是后来他同别的女人结了婚。他...      1       140  一般幽默   \n",
       "\n",
       "                                              pinyin  \n",
       "0  che1 shang4 yu4 jian4 yi1 dui4 shuang1 bao1 ta...  \n",
       "1  tu2 shu1 guan3 zi4 xi2 shi4 mei3 ge4 zhuo1 mia...  \n",
       "2  yi1 ge4 bao4 tong2 zai4 da4 jie1 shang4 gao1 s...  \n",
       "3  fa3 guan1 dui4 bei4 gao4 shuo1 ： ni3 bu4 dan4 ...  \n",
       "4  da4 ren2 ， yuan2 gao4 zai4 fa3 ting2 shang4 sh...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"./data/test_dataset.csv\")\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = test_dataset[\"joke\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 生成回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_content, content):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "max_retries = 3\n",
    "system_content = \"你是一位语言学家，请你判断以下文本的幽默程度，输出分类只能是'弱幽默','一般幽默','强幽默'三者之一，不能包含其它符号。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：同时拥有两种或以上幽默水平\n"
     ]
    }
   ],
   "source": [
    "def detect_humor_level(text):\n",
    "    humor_levels = [\"弱幽默\", \"一般幽默\", \"强幽默\"]\n",
    "    detected_levels = []\n",
    "\n",
    "    for level in humor_levels:\n",
    "        if level in text:\n",
    "            detected_levels.append(level)\n",
    "\n",
    "    if len(detected_levels) == 1:\n",
    "        return detected_levels[0]\n",
    "    elif len(detected_levels) > 1:\n",
    "        return \"警告：同时拥有两种或以上幽默水平\"\n",
    "    else:\n",
    "        return \"警告：未检测到幽默水平\"\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "text = \"这个笑话真的很弱幽默，但又有一些一般幽默的味道。\"\n",
    "print(detect_humor_level(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(len(content_list)):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        response = generate_response(system_content, content_list[i])\n",
    "        if response in [\"弱幽默\", \"一般幽默\", \"强幽默\"]:\n",
    "            responses.append(response)\n",
    "            break\n",
    "        else:\n",
    "            response_tmp = detect_humor_level(response)\n",
    "            if response_tmp in [\"弱幽默\", \"一般幽默\", \"强幽默\"]:\n",
    "                responses.append(response_tmp)\n",
    "                break\n",
    "            else:\n",
    "                retries += 1\n",
    "    else:\n",
    "        print(f\"回答{i}生成错误次数超过上限！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "预测标签\n",
       "强幽默     2324\n",
       "弱幽默      732\n",
       "一般幽默      74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"预测标签\"] = responses\n",
    "test_dataset[\"预测标签\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2390/3744847549.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_dataset[\"pred_label\"] = test_dataset[\"预测标签\"].replace(\n"
     ]
    }
   ],
   "source": [
    "test_dataset[\"pred_label\"] = test_dataset[\"预测标签\"].replace(\n",
    "    {\"弱幽默\": 0, \"一般幽默\": 1, \"强幽默\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate(targets, outputs):\n",
    "    conf = metrics.confusion_matrix(targets, outputs)\n",
    "    data = pd.DataFrame(\n",
    "        conf,\n",
    "        columns=[\"weak humor\", \"general humor\", \"strong humor\"],\n",
    "        index=[\"weak humor\", \"general humor\", \"strong humor\"],\n",
    "    )\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(data, cmap=plt.cm.Blues, annot=True, fmt=\"d\")\n",
    "    plt.xlabel(\"Predict label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "    plt.show()\n",
    "    print(\"Classification Report\")\n",
    "    print(metrics.classification_report(targets, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_dataset[\"label\"], test_dataset[\"pred_label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 保存回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/prediction.csv\")\n",
    "data[\"zero-shot\"] = test_dataset[\"pred_label\"]\n",
    "data.to_csv(\"./data/prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[[\"label\", \"预测标签\", \"pred_label\"]].to_csv(\n",
    "    \"./data/test_prediction_qwen_zeroshot.csv\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
